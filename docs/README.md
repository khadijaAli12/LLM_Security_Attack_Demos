# LLM Security Attack Demonstrations

**Roll No:** BITF22M028

---

## **Project Idea**
This project demonstrates how Large Language Models (LLMs) can be tricked or misused, highlighting common security risks in a simple and visual way.

---

## **Project Summary (5 Demos)**
1. **Prompt Injection**  
   Tricks the AI to ignore its rules and reveal secrets.  

2. **Data Leakage**  
   Shows what happens when hidden or sensitive data is accidentally exposed.  

3. **Insecure Output Handling (XSS)**  
   Demonstrates unsafe execution of user input as code or HTML.  

4. **Denial of Service (DoS)**  
   Simulates system slowdown or crash due to excessive requests or heavy input.  

5. **Supply Chain Vulnerabilities**  
   Shows risks of using unsafe or untrusted external models or libraries.

---

## **Features**
- Web interface to test all 5 demos.  
- Safe and unsafe outputs are clearly highlighted.  
- Interactive demonstration of each vulnerability.  
- Easy-to-understand examples with minimal technical jargon.  

---

## **How to Run**
1. Open the `index.html` file in a browser.  
2. Select a demo from the dropdown.  
3. Enter your input in the textarea.  
4. Click **Run Demo** to see the output.  

> **Note:** The demos simulate vulnerabilities; no real sensitive data is exposed.

---

## **Conclusion**
LLMs are powerful but vulnerable. This project demonstrates common risks and emphasizes the importance of secure usage and awareness.
