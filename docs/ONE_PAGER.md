LLM SECURITY ATTACK DEMONSTRATIONS

Roll No: BITF22M028

Actual Idea:

Demonstrate how Large Language Models (LLMs) can be tricked or misused and show the risks in simple examples.

Project Summary (5 Demos):

Prompt Injection: Tricks AI to ignore rules and reveal secrets.

Data Leakage: AI accidentally exposes hidden or sensitive data.

Insecure Output Handling (XSS): User input runs harmful scripts if not sanitized.

Denial of Service (DoS): System slows down or crashes when overloaded.

Supply Chain Vulnerabilities: Using unsafe external models or libraries may be risky.

What is Done & Working:

Demo scripts for all 5 attacks are ready and functional.

Web interface to input text and run demos.

Shows safe vs unsafe outputs clearly.

Simple visualization of vulnerabilities.

No sensitive data is actually exposed in demos.

Conclusion:

LLMs are powerful but can be misused. Awareness and safe handling are essential to prevent attacks.